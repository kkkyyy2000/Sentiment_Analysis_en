{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#ドライブをマウント\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "#モデルの再現性の担保（毎回の学習結果が異なることを防ぐため）\n",
    "def seed_everything(SEED=42):\n",
    "  os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "  os.environ['TF_DETERMINISTIC_OPS'] = 'true'\n",
    "  os.environ['TF_CUDNN_DETERMINISTIC'] = 'true'\n",
    "\n",
    "  np.random.seed(SEED)\n",
    "  rn.seed(SEED)\n",
    "  tf.random.set_seed(SEED)\n",
    "\n",
    "  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "  K.set_session(sess)\n",
    "\n",
    "seed_everything(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの読み込み\n",
    "df = pd.read_csv(\"./drive/MyDrive/Sentiment_Analysis/training_cleaned.csv\", header=None)\n",
    "df.head()\n",
    "\n",
    "#正解ラベルとツイートを抽出\n",
    "corpus = df[[0, 5]]\n",
    "corpus.info()\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for l, s in zip(corpus[0], corpus[5]):\n",
    "  sentences.append(s)\n",
    "  if l == 4:\n",
    "    l = 1\n",
    "    labels.append(l)\n",
    "  else:\n",
    "    labels.append(l)\n",
    "\n",
    "#学習、検証、テストデータに分割\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(sentences, labels, test_size=0.1, random_state=3407)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full, test_size=0.2, random_state=3407)\n",
    "\n",
    "print(f\"train_data_size : {len(x_train)}\")\n",
    "print(f\"valid_data_size : {len(x_valid)}\")\n",
    "print(f\"test_data_size : {len(x_test)}\")\n",
    "\n",
    "#単語ID化、パディング\n",
    "oov_tok = \"<OOV>\"\n",
    "padding_type=\"post\"\n",
    "max_length=16\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "word_index = tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "print(f\"tokenの種類：{len(word_index)}\")\n",
    "print(f\"パディング前のシーケンスの長さ：{len(train_sequences[0])}\")\n",
    "print(f\"パディング後のシーケンスの長さ：{len(train_padded[0])}\")\n",
    "\n",
    "valid_sequences = tokenizer.texts_to_sequences(x_valid)\n",
    "valid_padded = pad_sequences(valid_sequences, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "#正解ラベルをndarrayに変換する\n",
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ベースモデル構築、学習、評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル構築\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 16\n",
    "model = tf.keras.models.Sequential([\n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "#モデル学習\n",
    "num_epochs = 5\n",
    "num_batchs = 100\n",
    "history = model.fit(train_padded, y_train, epochs=num_epochs, batch_size=num_batchs, validation_data=(valid_padded, y_valid))\n",
    "\n",
    "#モデルの評価\n",
    "model.evaluate(test_padded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習プロセス可視化（ベースモデル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習プロセス可視化\n",
    "acc=history.history[\"accuracy\"]\n",
    "val_acc=history.history[\"val_accuracy\"]\n",
    "loss=history.history[\"loss\"]\n",
    "val_loss=history.history[\"val_loss\"]\n",
    "epochs=range(len(acc))\n",
    "\n",
    "acc_title_n = \"Training and validation accuracy\"\n",
    "loss_title_n = \"Training and validation loss\"\n",
    "acc_label_n = \"Accuracy\"\n",
    "loss_label_n = \"Loss\"\n",
    "acc_legend_n = [\"Accuracy\", \"Validation Accuracy\"]\n",
    "loss_legend_n = [\"Loss\", \"Validation Loss\"]\n",
    "\n",
    "def performance_plot(train, valid, title_n, label_n, legend_n):\n",
    "    plt.plot(epochs, train, \"r\")\n",
    "    plt.plot(epochs, valid, \"b\")\n",
    "    plt.title(title_n)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(label_n)\n",
    "    plt.legend(legend_n)\n",
    "    plt.show()\n",
    "\n",
    "performance_plot(acc, val_acc, acc_title_n, acc_label_n, acc_legend_n)\n",
    "performance_plot(loss, val_loss, loss_title_n, loss_label_n, acc_legend_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 過学習対策モデルの構築、学習、評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル構築\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 16\n",
    "model2 = tf.keras.models.Sequential([\n",
    "                            tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "                            tf.keras.layers.Dropout(0.2),\n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model2.summary()\n",
    "\n",
    "#モデル学習\n",
    "num_epochs = 5\n",
    "num_batchs = 100\n",
    "history2 = model2.fit(train_padded, y_train, epochs=num_epochs, batch_size=num_batchs, validation_data=(valid_padded, y_valid))\n",
    "\n",
    "#モデルの評価\n",
    "model2.evaluate(test_padded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習プロセス可視化（過学習対策モデル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=history.history[\"accuracy\"]\n",
    "val_acc=history.history[\"val_accuracy\"]\n",
    "loss=history.history[\"loss\"]\n",
    "val_loss=history.history[\"val_loss\"]\n",
    "\n",
    "d_acc=history2.history[\"accuracy\"]\n",
    "d_val_acc=history2.history[\"val_accuracy\"]\n",
    "d_loss=history2.history[\"loss\"]\n",
    "d_val_loss=history2.history[\"val_loss\"]\n",
    "\n",
    "epochs=range(len(acc))\n",
    "epochs2=range(len(d_acc))\n",
    "\n",
    "def acc_plot(epochs, epochs2, acc, val_acc, d_acc, d_val_acc):\n",
    "\n",
    "  plt.plot(epochs, acc, \"r\")\n",
    "  plt.plot(epochs, val_acc, \"b\")\n",
    "  plt.plot(epochs2, d_acc, \"m\")\n",
    "  plt.plot(epochs2, d_val_acc, \"c\")\n",
    "  \n",
    "  plt.title(\"Training and validation accuracy\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.legend([\"Acc\", \"Valid_Acc\", \"Dropout_Acc\", \"Dropout_Val_Acc\"])\n",
    "  plt.show()\n",
    "\n",
    "def loss_plot(epochs, epochs2, loss, val_loss, d_loss, d_val_loss):\n",
    "  plt.plot(epochs, loss, \"r\")\n",
    "  plt.plot(epochs, val_loss, \"b\")\n",
    "  plt.plot(epochs2, d_loss, \"m\")\n",
    "  plt.plot(epochs2, d_val_loss, \"c\")\n",
    "\n",
    "  plt.title(\"Training and validation loss\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.legend([\"Loss\", \"Validation Loss\", \"Dropout_Loss\", \"Dropout_Val_Loss\"])\n",
    "  plt.show()\n",
    "\n",
    "acc_plot(epochs, epochs2, acc, val_acc, d_acc, d_val_acc)\n",
    "loss_plot(epochs, epochs2, loss, val_loss, d_loss, d_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混同行列\n",
    "def pred_mat(test_padded, y_test, model):\n",
    "  predict_prob=model.predict(test_padded)\n",
    "  predict_classes=np.argmax(predict_prob,axis=1)\n",
    "  y_pred = []\n",
    "  for score in predict_prob:\n",
    "    if score < 0.5:\n",
    "      y_pred.append(0)\n",
    "    elif score > 0.5:\n",
    "      y_pred.append(1)\n",
    "  c_matrix = confusion_matrix(y_test, y_pred, labels=[0, 1])#混同行列のラベルの順序を指定\n",
    "  return y_pred, c_matrix\n",
    "\n",
    "#混同行列_DataFrame\n",
    "def make_cm(matrix, columns):\n",
    "    n = len(columns)\n",
    "    act = [\"正解データ\"] * n\n",
    "    pred = [\"予測結果\"] * n\n",
    "    cm = pd.DataFrame(matrix, columns=[pred, columns], index=[act, columns])\n",
    "    return cm\n",
    "\n",
    "y_pred2, c_matrix2 = pred_mat(test_padded, y_test, model2)\n",
    "cm = make_cm(c_matrix2, [\"NEGATIVE\", \"POSITIVE\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# サンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False_Negative_Texts\n",
    "samp_index = []\n",
    "index = 0\n",
    "\n",
    "for label, pred in zip(y_test, y_pred2):\n",
    "  if label == 1 and pred ==0:\n",
    "    samp_index.append(index)\n",
    "    index += 1\n",
    "  else:\n",
    "    index +=1\n",
    "\n",
    "#False_Negative_Texts_DataFrame\n",
    "x_test_numpy = np.array(x_test)\n",
    "x_test_samples = x_test_numpy[samp_index]\n",
    "sample_df = pd.DataFrame(x_test_samples, columns=[\"sample_text\"])\n",
    "sample_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False_Positive_Texts\n",
    "samp_index2 = []\n",
    "index2 = 0\n",
    "\n",
    "for label, pred in zip(y_test, y_pred2):\n",
    "  if label == 0 and pred ==1:\n",
    "    samp_index2.append(index2)\n",
    "    index2 += 1\n",
    "  else:\n",
    "    index2 +=1\n",
    "\n",
    "#False_Positive_Texts_DataFrame\n",
    "x_test_numpy2 = np.array(x_test)\n",
    "x_test_samples2 = x_test_numpy2[samp_index2]\n",
    "sample_df2 = pd.DataFrame(x_test_samples2, columns=[\"sample_text\"])\n",
    "sample_df2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# エラーにおける頻出単語Top20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False_Negative_Textsの単語を抽出\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def filter_stop_words(sentences, stop_words):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        new_sent = [word for word in sentence.split() if word not in stop_words]\n",
    "        sentences[i] = \" \".join(new_sent)\n",
    "    return sentences\n",
    "\n",
    "#stop_wordの削除\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sw_x_test = filter_stop_words(x_test_samples, stop_words)\n",
    "\n",
    "#stemming\n",
    "snowball = SnowballStemmer(language=\"english\")\n",
    "sw_x_test_texts = [text for text in sw_x_test]\n",
    "x_test_words = \" \".join(sw_x_test_texts).split()\n",
    "clean_test_words = [snowball.stem(t) for t in x_test_words]\n",
    "\n",
    "#トークン化\n",
    "nltk.download(\"punkt\")\n",
    "w_list = []\n",
    "for t in clean_test_words:\n",
    "  t = nltk.word_tokenize(t)\n",
    "  for w in t:\n",
    "    w_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False_Positive_Textsの単語を抽出\n",
    "def filter_stop_words(sentences, stop_words):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        new_sent = [word for word in sentence.split() if word not in stop_words]\n",
    "        sentences[i] = \" \".join(new_sent)\n",
    "    return sentences\n",
    "\n",
    "#stop_wordの削除\n",
    "stop_words2 = set(stopwords.words(\"english\"))\n",
    "sw_x_test2 = filter_stop_words(x_test_samples2, stop_words2)\n",
    "#stemming\n",
    "snowball2 = SnowballStemmer(language=\"english\")\n",
    "sw_x_test_texts2 = [text for text in sw_x_test2]\n",
    "x_test_words2 = \" \".join(sw_x_test_texts2).split()\n",
    "clean_test_words2 = [snowball2.stem(t) for t in x_test_words2]\n",
    "\n",
    "#トークン化\n",
    "w_list2 = []\n",
    "for t in clean_test_words2:\n",
    "  t = nltk.word_tokenize(t)\n",
    "  for w in t:\n",
    "    w_list2.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = collections.Counter(w_list)\n",
    "c2 = collections.Counter(w_list2)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8))\n",
    "sns.countplot(y=w_list,order=[i[0] for i in c.most_common(20)], ax=ax1)\n",
    "sns.countplot(y=w_list2,order=[i[0] for i in c2.most_common(20)], ax=ax2)\n",
    "ax1.set_title(\"Pred_Negative\", fontsize=20)\n",
    "ax2.set_title(\"Pred_Positive\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感情スコア算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "\n",
    "def decode_sentiment(score, include_neutral=True):\n",
    "    if include_neutral:        \n",
    "        label = NEUTRAL\n",
    "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
    "            label = NEGATIVE\n",
    "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "            label = POSITIVE\n",
    "        return label\n",
    "    else:\n",
    "        return NEGATIVE if score < 0.5 else POSITIVE\n",
    "\n",
    "def predict(text, model, include_neutral=True):\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=max_length, padding=padding_type)\n",
    "    score = model.predict([x_test])\n",
    "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
    "    results = f\"label : {label}, score : {float(score):.2f}, text : {text}\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 記号の有無"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#記号の有無（正解ラベル：ポジティブ）\n",
    "print(predict(\"@amandadoan you'll get the job\", model2))\n",
    "print(predict(\"amandadoan you'll get the job\", model2))\n",
    "print(predict(\"you'll get the job\", model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLの有無"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#①URLの有無（正解ラベル：ポジティブ）\n",
    "print(predict(\"i always dance alone in my room http://tumblr.com/xyx1xdji5\", model2))\n",
    "print(predict(\"i always dance alone in my room\", model2))\n",
    "\n",
    "#②URLの有無（正解ラベル：ネガティブ）\n",
    "print(predict(\"Cherry tree update: But the first day of full bloom also brings the first falling blossom http://twitpic.com/3phky\", model2))\n",
    "print(predict(\"Cherry tree update: But the first day of full bloom also brings the first falling blossom\", model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'mの有無"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'mの有無（正解ラベル：ポジティブ）\n",
    "print(predict(\"Oh man oh man. I found my old CD's. I'm listening to Underoath and Saosin's old schtuff\", model2))\n",
    "print(predict(\"Oh man oh man. I found my old CD's. I listening to Underoath and Saosin's old schtuff\", model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quotの有無"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quotの有無（正解ラベル：ネガティブ）\n",
    "print(predict(\"I AM going to bed this time.. Apologies for the many &quot;colourful&quot; tweets, gona stick some Wilco or Grizzly Bear on to calm down..\", model2))\n",
    "print(predict(\"I AM going to bed this time.. Apologies for the many colourful tweets, gona stick some Wilco or Grizzly Bear on to calm down..\", model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数字の有無"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#①数字の有無（正解ラベル：ポジティブ）\n",
    "print(predict(\"will be moving on june 10.\", model2))\n",
    "print(predict(\"will be moving on june.\", model2))\n",
    "\n",
    "#②数字の有無（正解ラベル：ネガティブ）\n",
    "print(predict(\"I cannot wait til summer. 14 more days\", model2))\n",
    "print(predict(\"I cannot wait til summer. more days\", model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文脈を考慮した予測かどうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文脈の確認（正解ラベル：ポジティブ）\n",
    "print(predict(\"cannot wait for my date tonight... and this weekend. I so bad\", model2))\n",
    "print(predict(\"cannot wait for my date tonight... and this weekend. I so\", model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの保存と読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの保存と読み込み\n",
    "#model2.save(\"./model.h5\")\n",
    "#model2 = tf.keras.models.load_model(\"\"./drive/MyDrive/Sentiment_Analysis/model.h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
